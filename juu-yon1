8

PROGRAM: 
from sklearn.datasets import fetch_20newsgroups 
from sklearn.feature_extraction.text import TfidfVectorizer  
from sklearn.decomposition import TruncatedSVD 
from sklearn.metrics.pairwise import cosine_similarity  
import numpy as np 
# Step 1: Load dataset (subset for speed) 
categories = ['sci.space', 'rec.sport.hockey', 'comp.graphics'] 
newsgroups = fetch_20newsgroups(subset='train', categories=categories, remove=('headers',  
'footers', 'quotes')) 
# Step 2: TF-IDF Vectorization 
vectorizer = TfidfVectorizer(stop_words='english', max_features=1000)  
X_tfidf = vectorizer.fit_transform(newsgroups.data) 
print(f"Original TF-IDF shape: {X_tfidf.shape}") # (docs x terms) 
# Step 3: SVD for LSI (Latent Semantic Indexing) 
k = 100 # number of latent dimensions  
svd = TruncatedSVD(n_components=k)  
X_lsi = svd.fit_transform(X_tfidf) 
print(f"Reduced LSI shape: {X_lsi.shape}") # (docs x k topics) 
# Step 4: Show similarity between some documents 
def show_similar_docs(query_idx, top_n=5):
similarities = cosine_similarity([X_lsi[query_idx]], X_lsi)[0]  
top_indices = similarities.argsort()[::-1][1:top_n+1] 
print(f"\nQuery Document {query_idx}:\n{newsgroups.data[query_idx][:300]}...\n")  
print("Top similar documents:") 
for i in top_indices: 
print(f"\nDoc #{i} (Similarity: {similarities[i]:.3f}):\n{newsgroups.data[i][:300]}...") 
# Example: Show top 5 similar documents to doc #0 
show_similar_docs(query_idx=0, top_n=5)


-------------------------------------------------------------------

5

import os import nltk 
from nltk.corpus import stopwords 
from nltk.tokenize import word_tokenize  
from nltk.stem import PorterStemmer  
from collections import defaultdict import json 
nltk.download('punkt') 
 nltk.download('stopwords') 
def preprocess(text): 
text = text.lower() 
tokens = word_tokenize(text) 
stop_words = set(stopwords.words('english')) stemmer = PorterStemmer() 
words = [stemmer.stem(word) for word in tokens if word.isalnum() and word not in 
stop_words] 
return words documents = {} 
for filename in os.listdir(): 
if filename.endswith(".txt"): 
with open(filename, 'r', encoding='utf-8', errors='ignore') as f:  
text = f.read() 
documents[filename] = preprocess(text) 
print(f"Total documents loaded: {len(documents)}")  
inverted_index = defaultdict(set) 
for doc_id, words in documents.items(): 
for word in set(words): # avoid duplicates per document  
inverted_index[word].add(doc_id) 
vocab_size = len(inverted_index)  
print(f"\nVocabulary Size: {vocab_size} words")  
print("\nSample inverted index terms:") 
for term in list(inverted_index)[:10]:  
print(f"{term}: {sorted(inverted_index[term])}")

-------------------------------------------------------------------------
2a
import nltk 
from nltk.corpus import stopwords 
from nltk.tokenize import word_tokenize from 
nltk.stem import PorterStemmer nltk.download('punkt') 
nltk.download('stopwords')
def preprocess_text(text):  
text = text.lower() 
words = word_tokenize(text) 
stop_words = set(stopwords.words('english')) 
filtered_words = [word for word in words if word.isalnum() and word not in stop_words] 
stemmer = PorterStemmer() 
stemmed_words = [stemmer.stem(word) for word in filtered_words]  
return stemmed_words 
text = "Machine learning algorithms are revolutionizing the world of artificial intelligence." 
print("Orginal Text:",text)  
processed = preprocess_text(text)  
processed_text = ' '.join(processed)  
print("Processed Text:", processed_text)  
print("Preprocessed Words:", processed)
----------------------
2b
pip install --upgrade numpy scipy network  
import networkx as nx 
# Example scholarly citation network 
# Each node is a paper, edges represent citations  
citations = { 
"Paper1": ["Paper2", "Paper3"], 
"Paper2": ["Paper3"], 
"Paper3": ["Paper1"], 
"Paper4": ["Paper2", "Paper3"], 
"Paper5": ["Paper3", "Paper4"] 
} 
 
# Build directed graph G = nx.DiGraph() 
for paper, cited_papers in citations.items():  
for cited in cited_papers: 
G.add_edge(paper, cited) 
# Compute PageRank manually (no scipy backend needed) pagerank_scores = nx.pagerank(G, 
alpha=0.85, max_iter=100) print("\n)› |‘ PageRank Scores:") 
for paper, score in pagerank_scores.items(): 
print(f"{paper}: {score:.4f}")



